{"code": "# Neural VM Experiment: simple_gpt\n# Generated by Neural VM Builder\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass, field\nimport time\n\n# Atomic Components (placeholder implementations)\nclass SimpleLinearAtom(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n    def forward(self, x): return self.linear(x)\n\nclass SimpleAttentionAtom(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n    def forward(self, x): return self.attn(x, x, x)[0]\n\nclass EmbeddingAtom(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n    def forward(self, x): return self.embed(x.long())\n\nclass PositionalEncodingAtom(nn.Module):\n    def __init__(self, max_len, embed_dim):\n        super().__init__()\n        self.max_len = max_len\n        self.embed_dim = embed_dim\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe.unsqueeze(0))\n    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n\nclass LayerNormAtom(nn.Module):\n    def __init__(self, normalized_shape):\n        super().__init__()\n        self.ln = nn.LayerNorm(normalized_shape)\n    def forward(self, x): return self.ln(x)\n\nclass ActivationAtom(nn.Module):\n    def __init__(self, activation_type='relu'):\n        super().__init__()\n        self.act = nn.GELU() if activation_type == 'gelu' else nn.ReLU()\n    def forward(self, x): return self.act(x)\n\nclass AddAtom(nn.Module):\n    def __init__(self): super().__init__()\n    def forward(self, x, y=None): return x + y if y is not None else x\n\nclass DropoutAtom(nn.Module):\n    def __init__(self, dropout_rate=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout_rate)\n    def forward(self, x): return self.dropout(x)\n\n# Setup\ninput_tokens = np.random.randint(0, 1000, (1, 10))\n\n# Component definitions\nembedding = EmbeddingAtom(50257, 512)\npos_encoding = PositionalEncodingAtom(512, 512)\nattention = SimpleAttentionAtom(512, 8)\nadd1 = AddAtom()\nattn_norm = LayerNormAtom(512)\nffn_linear1 = SimpleLinearAtom(512, 2048, True)\nffn_activation = ActivationAtom('gelu')\nffn_linear2 = SimpleLinearAtom(2048, 512, True)\nadd2 = AddAtom()\nffn_norm = LayerNormAtom(512)\ndropout = DropoutAtom(0.1)\nfinal_norm = LayerNormAtom(512)\noutput_projection = SimpleLinearAtom(512, 50257, False)\n\n# Execution\nembedded = embedding(torch.from_numpy(input_tokens))\nx = pos_encoding(embedded)\nattn_out = attention(x)\nx_residual = add1(x, attn_out)\nx = attn_norm(x_residual)\nffn_out = ffn_linear1(x)\nffn_out = ffn_activation(ffn_out)\nffn_out = ffn_linear2(ffn_out)\nx_residual = add2(x, ffn_out)\nx = ffn_norm(x_residual)\nx = dropout(x)\nx = final_norm(x)\nlogits = output_projection(x)\n\nprint('Experiment simple_gpt completed successfully!')\nprint(f'Output shape: {logits.shape}')\n"}